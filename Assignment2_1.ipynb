{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachi097/Agrisense/blob/master/Assignment2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfsuonrWtD-3"
      },
      "source": [
        "# Assignment 2: Text Classification with Logistic Regression\n",
        "\n",
        "This time, we will dive into **text classification** --one of the most popular NLP tasks!\n",
        "\n",
        "In simple terms, **supervised** text classification is the task of taking a string and assigning it a **label** or a **class**. These labels can be as varied as we can possibly imagine. For example, is a string spam or not? Does it contain hate speech or not? Does it communicate a positive emotion or a negative one?\n",
        "\n",
        "As you can see, text classification has many potential applications in this digital age. Given the massive number of strings produced every day, no human is capable of reading and assigning them all a label. Therefore, we want to come up with ways to classify them automatically. This is where **logistic regression (LR)** comes in handy, since it is a **machine learning algorithm** that predicts labels on **unseen inputs**.\n",
        "\n",
        "In this assignment, we will train a LR model that will help us determine whether a Reddit post contains a \"controversial\" opinion or not.\n",
        "\n",
        "*Some of the text in this Assignment is taken from the [book](https://web.stanford.edu/~jurafsky/slp3/5.pdf)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8OOsARgCbz"
      },
      "source": [
        "## Text classification\n",
        "\n",
        "This NLP task can be subdivided in the following **substasks**: sentiment analysis, spam detection, language identiﬁcation, and authorship attribution.\n",
        "\n",
        "**Sentiment analysis** classiﬁes a text as reﬂecting the positive or negative orientation (sentiment) that a writer expresses toward some object.\n",
        "\n",
        "There are many classification algorithms. The most popular ones are naive Bayes, logistic regression, random forests, and suport vector machines. In this assignment we will only make use of logistic regression.\n",
        "\n",
        "Classiﬁers are **trained** using distinct training, dev, and test sets. Then, they are **evaluated** with various metrics. The most popular ones are  **precision**, **recall**, **accuracy** and **F1 metric**.\n",
        "\n",
        "Statistical signiﬁcance tests should be used to determine whether we can be conﬁdent that one version of a classiﬁer is better than another.\n",
        "\n",
        "## Logistic regression (LR)\n",
        "\n",
        "Logistic regression can be used to classify an observation into one of two classes (like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.\n",
        "  \n",
        "The main idea behind LR is computing the probability of assigning a document $d$ the probability of having a class $c$. This is,\n",
        "$$\n",
        "P(c \\mid d) =P(y \\mid x)\n",
        "$$\n",
        "\n",
        "LR (and other probabilistic machine learning classifiers) have the following components:\n",
        "  1.  A **feature representation** of the input. For each input observation $x^{(i)}$, this will be a vector of features $\\left[x_1, x_2, \\ldots, x_n\\right]$. We will generally refer to feature $i$ for input $x^{(j)}$ as $x_i^{(j)}$, sometimes simplified as $x_i$, but we will also see the notation $f_i, f_i(x)$, or, for multiclass classification, $f_i(c, x)$.\n",
        "  2.  A **classification function** that computes $\\hat{y}$, the estimated class, via $p(y \\mid x)$. In the next section we will introduce the **sigmoid** and **softmax** tools for classification.\n",
        "  3.  An **objective function** for learning, usually involving minimizing error on training examples. We will use the **cross-entropy loss** function.\n",
        "  4.  An algorithm for **optimizing** the objective function. We will use the **stochastic gradient descent** algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l8p4K22gCb0"
      },
      "source": [
        "## 0. Let's look at our data\n",
        "\n",
        "Before the feature representation step, we need to load our *corpus* and see whats in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrG5SvozgCb1",
        "outputId": "55a495d6-985a-4f54-973e-48e7bbe3db8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.20.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "# this will install Polars in our notebook.\n",
        "# Polars is a useful data wrangling library.\n",
        "! pip install polars\n",
        "\n",
        "# this will install Pytorch, a popular ML framework.\n",
        "! pip install torch torchvision torchaudio\n",
        "\n",
        "# numpy is a popular scientific computing library.\n",
        "! pip install numpy\n",
        "\n",
        "# tqdm makes your loops show a smart progress meter\n",
        "# source: https://pypi.org/project/tqdm/\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RMhXfuigCb2"
      },
      "source": [
        "The following block of code will read our *corpus* into memory and print a brief summary of the data contained in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "HnI8U1ZigCb2",
        "outputId": "69f3636e-d48b-4d42-ed1c-f9b08170d4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (9, 25)\n",
              "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
              "│ describe  ┆ comment_i ┆ score     ┆ self_text ┆ … ┆ post_upvo ┆ post_thum ┆ post_tota ┆ post_cre │\n",
              "│ ---       ┆ d         ┆ ---       ┆ ---       ┆   ┆ te_ratio  ┆ bs_ups    ┆ l_awards_ ┆ ated_tim │\n",
              "│ str       ┆ ---       ┆ f64       ┆ str       ┆   ┆ ---       ┆ ---       ┆ received  ┆ e        │\n",
              "│           ┆ str       ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ ---       ┆ ---      │\n",
              "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ f64       ┆ str      │\n",
              "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
              "│ count     ┆ 10000     ┆ 10000.0   ┆ 10000     ┆ … ┆ 10000.0   ┆ 10000.0   ┆ 10000.0   ┆ 10000    │\n",
              "│ null_coun ┆ 0         ┆ 0.0       ┆ 0         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0        │\n",
              "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ mean      ┆ null      ┆ 13.6226   ┆ null      ┆ … ┆ 0.810165  ┆ 2223.6118 ┆ 0.0       ┆ null     │\n",
              "│ std       ┆ null      ┆ 116.75857 ┆ null      ┆ … ┆ 0.202442  ┆ 4348.5458 ┆ 0.0       ┆ null     │\n",
              "│           ┆           ┆ 2         ┆           ┆   ┆           ┆ 72        ┆           ┆          │\n",
              "│ min       ┆ eeyxlnh   ┆ -199.0    ┆           ┆ … ┆ 0.04      ┆ 0.0       ┆ 0.0       ┆ 2019-01- │\n",
              "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 25       │\n",
              "│           ┆           ┆           ┆ &gt;1.    ┆   ┆           ┆           ┆           ┆ 22:55:15 │\n",
              "│           ┆           ┆           ┆ America   ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ is        ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ leading   ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ the …     ┆   ┆           ┆           ┆           ┆          │\n",
              "│ 25%       ┆ null      ┆ 0.0       ┆ null      ┆ … ┆ 0.72      ┆ 99.0      ┆ 0.0       ┆ null     │\n",
              "│ 50%       ┆ null      ┆ 2.0       ┆ null      ┆ … ┆ 0.9       ┆ 457.0     ┆ 0.0       ┆ null     │\n",
              "│ 75%       ┆ null      ┆ 6.0       ┆ null      ┆ … ┆ 0.96      ┆ 2465.0    ┆ 0.0       ┆ null     │\n",
              "│ max       ┆ ki6s9k7   ┆ 5129.0    ┆ 🦀🦀🦀🦀  ┆ … ┆ 1.0       ┆ 55746.0   ┆ 0.0       ┆ 2024-01- │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆ 16       │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆ 20:44:05 │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀  ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆           ┆ 🦀🦀🦀🦀… ┆   ┆           ┆           ┆           ┆          │\n",
              "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (9, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>comment_id</th><th>score</th><th>self_text</th><th>subreddit</th><th>created_time</th><th>post_id</th><th>author_name</th><th>controversiality</th><th>ups</th><th>downs</th><th>user_is_verified</th><th>user_account_created_time</th><th>user_awardee_karma</th><th>user_awarder_karma</th><th>user_link_karma</th><th>user_comment_karma</th><th>user_total_karma</th><th>post_score</th><th>post_self_text</th><th>post_title</th><th>post_upvote_ratio</th><th>post_thumbs_ups</th><th>post_total_awards_received</th><th>post_created_time</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;10000&quot;</td><td>10000.0</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>&quot;10000&quot;</td><td>&quot;10000&quot;</td><td>10000.0</td><td>10000.0</td><td>10000.0</td><td>&quot;10000&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>13.6226</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.5</td><td>13.6226</td><td>0.0</td><td>null</td><td>null</td><td>828.2566</td><td>314.1331</td><td>20593.8092</td><td>76800.3118</td><td>98536.5107</td><td>2223.6118</td><td>null</td><td>null</td><td>0.810165</td><td>2223.6118</td><td>0.0</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>116.758572</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.500025</td><td>116.758572</td><td>0.0</td><td>null</td><td>null</td><td>3522.184763</td><td>1818.285791</td><td>227349.618559</td><td>162506.111621</td><td>302778.131495</td><td>4348.545872</td><td>null</td><td>null</td><td>0.202442</td><td>4348.545872</td><td>0.0</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;eeyxlnh&quot;</td><td>-199.0</td><td>&quot;\n",
              "\n",
              "&amp;gt;1. Ameri…</td><td>&quot;AskReddit&quot;</td><td>&quot;2019-01-25 23:…</td><td>&quot;1005d6u&quot;</td><td>&quot;----Dongers&quot;</td><td>0.0</td><td>-199.0</td><td>0.0</td><td>&quot;False&quot;</td><td>&quot;&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-100.0</td><td>-99.0</td><td>0.0</td><td>&quot;&quot;</td><td>&quot;&quot;.... and now …</td><td>0.04</td><td>0.0</td><td>0.0</td><td>&quot;2019-01-25 22:…</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>31.0</td><td>5362.0</td><td>6406.0</td><td>99.0</td><td>null</td><td>null</td><td>0.72</td><td>99.0</td><td>0.0</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>2.0</td><td>0.0</td><td>null</td><td>null</td><td>140.0</td><td>0.0</td><td>544.0</td><td>22565.0</td><td>26363.0</td><td>457.0</td><td>null</td><td>null</td><td>0.9</td><td>457.0</td><td>0.0</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>6.0</td><td>0.0</td><td>null</td><td>null</td><td>598.0</td><td>57.0</td><td>4278.0</td><td>76263.0</td><td>88463.0</td><td>2465.0</td><td>null</td><td>null</td><td>0.96</td><td>2465.0</td><td>0.0</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;ki6s9k7&quot;</td><td>5129.0</td><td>&quot;🦀🦀🦀🦀🦀🦀🦀🦀🦀🦀🦀🦀🦀🦀…</td><td>&quot;uspolitics&quot;</td><td>&quot;2024-01-16 21:…</td><td>&quot;zziysm&quot;</td><td>&quot;zyzzogeton&quot;</td><td>1.0</td><td>5129.0</td><td>0.0</td><td>&quot;True&quot;</td><td>&quot;2024-01-16 16:…</td><td>149281.0</td><td>57047.0</td><td>1.4549215e7</td><td>3.76565e6</td><td>1.5517137e7</td><td>55746.0</td><td>&quot;💩💩💩💩💩&quot;</td><td>&quot;🚨 NEW: Trump&#x27;s…</td><td>1.0</td><td>55746.0</td><td>0.0</td><td>&quot;2024-01-16 20:…</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "import polars as pl\n",
        "\n",
        "df = pl.read_csv(\"data.csv\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxcmAe5fgCb3"
      },
      "source": [
        "As we can see, our DataFrame object contains a lot of data. We could create very complex **features** using all the columns, but for this assignment we will only consider the Reddit posts (strings) and their labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uI2tgaygCb3"
      },
      "source": [
        "### + 0.5 points - Create a subsample of the DataFrame created above\n",
        "\n",
        "Using the previously created `df` object, create a `df_subsampled` object that contains only the data in the columns \"self_text\" and \"controversiality\". Then, rename the column \"controversiality\" to \"label\". Finally, print the value distribution in the column \"label\". This will help us get an idea of our label distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAlhi5pBgCb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85471c28-c6dd-4f31-eab4-858bf7085f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (2, 2)\n",
            "┌───────┬───────┐\n",
            "│ label ┆ count │\n",
            "│ ---   ┆ ---   │\n",
            "│ i64   ┆ u32   │\n",
            "╞═══════╪═══════╡\n",
            "│ 0     ┆ 5000  │\n",
            "│ 1     ┆ 5000  │\n",
            "└───────┴───────┘\n"
          ]
        }
      ],
      "source": [
        "# Write your solution here.\n",
        "df_subsampled = df.select([\"self_text\", \"controversiality\"])\n",
        "df_subsampled = df_subsampled.rename({\"controversiality\": \"label\"})\n",
        "print(df_subsampled['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR2MISaqgCb4"
      },
      "source": [
        "*The expected output is a table of counts of each type of label in the dataset, i.e.:*\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dWL3_OgCb4"
      },
      "source": [
        "Lets take a look at two texts in our *corpus*. It's always a good idea to get an idea of what's in it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54jrI5N3gCb4",
        "outputId": "26839d7c-1c57-4c51-c5b1-464964c8bf13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sample with label 0 ---\n",
            "Don't forget that they have to obey every order given them by a Trump, no matter how illegal it is.\n",
            "\n",
            "--- Sample with label 1 ---\n",
            "Care to elaborate on which Trump policies Biden stopped using and how it affected the border?\n"
          ]
        }
      ],
      "source": [
        "def print_samples(df_subsampled):\n",
        "    sample_0 = df_subsampled.filter(pl.col(\"label\") == 0).sample(n=1)\n",
        "    sample_1 = df_subsampled.filter(pl.col(\"label\") == 1).sample(n=1)\n",
        "\n",
        "    print(\"--- Sample with label 0 ---\")\n",
        "    print(sample_0[\"self_text\"].to_list()[0])\n",
        "    print(\"\\n--- Sample with label 1 ---\")\n",
        "    print(sample_1[\"self_text\"].to_list()[0])\n",
        "\n",
        "print_samples(df_subsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaDJ_h0gCb4"
      },
      "source": [
        "### 0.5 pts - Train, dev and test sets\n",
        "\n",
        "Using the `df_subsampled` object, create three sets: training, development/validation, and test. The test set will be used in the evaluation phase, while the first two partitions will be used during the training phase. Remember to keep the test set entirely separate from the training and validation process. Train-dev-test sizes must be $80\\%$, $10\\%$, and $10\\%$ respectively. Feel free to use sklearn library to help you with this task - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JQjR0pwgCb4",
        "outputId": "cff6f262-4614-4053-9d3b-deed2c511c4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9ETaOI2gCb4"
      },
      "outputs": [],
      "source": [
        "# Write your solution here. Store your three sets in three\n",
        "# different variables called train, validation, test.\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainSet, test = train_test_split(df_subsampled, test_size=0.1, stratify=df_subsampled['label'])\n",
        "\n",
        "train, validation = train_test_split(trainSet, test_size=0.1111, stratify=trainSet['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4I-rEwzgCb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207329de-aa43-4b71-ed1b-166b9dcedf66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 2) (1000, 2) (1000, 2)\n"
          ]
        }
      ],
      "source": [
        "# Let's see the dimensions of our partitions.\n",
        "# DO NOT CHANGE THIS CODE.\n",
        "\n",
        "print(train.shape, validation.shape, test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhebjNrngCb5"
      },
      "source": [
        "## 1. Feature representation\n",
        "\n",
        "Now, we are ready to implement the first step in our learning pipeline.\n",
        "\n",
        "Since we need numerical inputs for our learning function, we have to transform our strings to numerical representations. Essentially, we need to find a way to make numbers encode some aspect of word or sentence meaning. There are many ways to do this. In this assignment, we will use the **TF-IDF algorithm** and the **CountVectorizer** method to create two sets of features (both sets will represent the same data). You can learn more about it [here](https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/#:~:text=Term%20Frequency%20%2D%20Inverse%20Document%20Frequency%20(TF%2DIDF)%20is,%2C%20relative%20to%20a%20corpus). The method involves multiplying two ratios - the term frequency and the inverse document frequency.\n",
        "\n",
        "The term frequency is the number of times a term appears in a document, divided by the total number of terms in the document. The inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents that contain the term.\n",
        "\n",
        "$TF|IDF(t, d, D) = TF(t, d) \\times IDF(t, D)$\n",
        "\n",
        "Where:\n",
        "\n",
        "$TF(t, d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}$\n",
        "\n",
        "$IDF(t, D) = \\log\\left(\\frac{N}{|\\{d \\in D : t \\in d\\}|}\\right)$\n",
        "\n",
        "Where:\n",
        "- $f_{t,d}$ is the number of times term $t$ appears in document $d$\n",
        "- $\\sum_{t' \\in d} f_{t',d}$ is the sum of the number of times each term appears in document $d$, which is basically the total number of terms in the document\n",
        "- $N$ is the number of documents in the corpus\n",
        "- $|\\{d \\in D : t \\in d\\}|$ is the number of documents in the corpus that contain term $t$\n",
        "\n",
        "\n",
        "This allows us to get a numerical representation that assigns higher values to the most important words in our *corpus* -- these are not necessarily the most frequent words. (think of the definite article \"the\". It might appear frequently in a corpus, but it does not provide a lot of semantic content)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnvxDBm1gCb5"
      },
      "source": [
        "### + 1.0 point - Complete the following function and implement the TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYoBPJVzgCb5"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "def find_vocabulary(rows: List[str]) -> List[str]:\n",
        "  vocabulary_list = list()\n",
        "  for row in rows:\n",
        "    tokens = row.split(\" \")\n",
        "    tokens = [ele for ele in tokens if ele.strip()]\n",
        "    vocabulary_list.extend(tokens)\n",
        "  vocabulary = set(vocabulary_list)\n",
        "  return sorted(vocabulary)\n",
        "\n",
        "def generate_docs(rows: List[str]) -> List[List[str]]:\n",
        "  docs = list()\n",
        "  for row in rows:\n",
        "    tokens = row.split(\" \")\n",
        "    tokens = [ele for ele in tokens if ele.strip()]\n",
        "    docs.append(tokens)\n",
        "  return docs\n",
        "\n",
        "def compute_tf_idf(docs: List[List[str]]) -> Dict[str, Dict[str, float]]:\n",
        "    token_hash = {}\n",
        "    token_frequency = {}\n",
        "    inverse_doc_frequency = {}\n",
        "\n",
        "    # Compute TF\n",
        "    i = 0\n",
        "    for doc in docs:\n",
        "      docKey = \"doc_\"+str(i)\n",
        "      token_frequency.update({docKey: {}})\n",
        "      doc_token_frequency = token_frequency[docKey]\n",
        "      total_tokens = len(doc)\n",
        "      for token in doc:\n",
        "        token_count = doc.count(token)\n",
        "        doc_token_frequency.update({token: (token_count / total_tokens)})\n",
        "        # preprocessing for IDF\n",
        "        if token not in token_hash:\n",
        "          token_hash[token] = set()\n",
        "          token_hash[token].add(i)\n",
        "        else:\n",
        "          token_hash[token].add(i)\n",
        "      i = i + 1\n",
        "\n",
        "    # Compute IDF\n",
        "    N = len(docs)\n",
        "    for doc in docs:\n",
        "      for token in doc:\n",
        "        inverse_doc_frequency.update({token: np.log(N / len(token_hash[token]))})\n",
        "\n",
        "    # Compute TF-IDF\n",
        "    tf_idf = copy.deepcopy(token_frequency)\n",
        "    for doc, doc_tf_idf in tf_idf.items():\n",
        "      for token in doc_tf_idf.keys():\n",
        "        doc_tf_idf[token] = doc_tf_idf[token] * inverse_doc_frequency[token]\n",
        "      tf_idf[doc] = doc_tf_idf\n",
        "\n",
        "    return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cEcW-oIgCb5"
      },
      "outputs": [],
      "source": [
        "vocabulary = find_vocabulary(df_subsampled['self_text'])\n",
        "docs = generate_docs(df_subsampled['self_text'])\n",
        "tf_idf = compute_tf_idf(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heyjPsWKgCb5"
      },
      "source": [
        "### + 1.0 point - Second set of features\n",
        "\n",
        "Lets generate a second set of features. This time, we will use `CountVectorizer`, a sklearn method that generates a matrix of token counts from the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nq8sZeTgCb5"
      },
      "outputs": [],
      "source": [
        "# Use the count vectorizer to convert the text data to a matrix of token counts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "count_vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkImfXDgCb5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def transform_features(count_vectorizer: CountVectorizer, train: pl.DataFrame, validation: pl.DataFrame, test: pl.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Fit the CountVectorizer to the train, validation and test  sets\n",
        "    count_vectorizer.fit(df_subsampled['self_text'])\n",
        "    train_features = count_vectorizer.transform(train['self_text'])\n",
        "    validation_features = count_vectorizer.transform(validation['self_text'])\n",
        "    test_features = count_vectorizer.transform(test['self_text'])\n",
        "    return train_features, validation_features, test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TS7MZvETgCb5"
      },
      "outputs": [],
      "source": [
        "train_features, validation_features, test_features = transform_features(count_vectorizer, train, validation, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzRI5vaagCb5"
      },
      "outputs": [],
      "source": [
        "# Convert the features to PyTorch tensors.\n",
        "# Tensors are a useful data structure in machine learning.\n",
        "# Their usefulness stems from the fact that they are \"n-dimensional vectors\" --this allows us\n",
        "# to conveniently store the features generated from text corpora.\n",
        "import torch\n",
        "\n",
        "def convert_to_tensors(train_features: np.ndarray, validation_features: np.ndarray, test_features: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    # Convert the features to PyTorch tensors\n",
        "    train_features_tensor = torch.from_numpy(train_features.todense()).float()\n",
        "    validation_features_tensor = torch.from_numpy(validation_features.todense()).float()\n",
        "    test_features_tensor = torch.from_numpy(test_features.todense()).float()\n",
        "    return train_features_tensor, validation_features_tensor, test_features_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gXdKullgCb5"
      },
      "outputs": [],
      "source": [
        "train_features_tensor, validation_features_tensor, test_features_tensor = convert_to_tensors(train_features, validation_features, test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqtvshX6gCb6"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to PyTorch tensors\n",
        "\n",
        "def convert_labels_to_tensors(train: pl.DataFrame, validation: pl.DataFrame, test: pl.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    # Convert the labels to PyTorch tensors\n",
        "    train_labels = torch.from_numpy(np.array(train['label']))\n",
        "    validation_labels = torch.from_numpy(np.array(validation['label']))\n",
        "    test_labels = torch.from_numpy(np.array(test['label']))\n",
        "    return train_labels, validation_labels, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uKujANXgCb6"
      },
      "outputs": [],
      "source": [
        "train_labels, validation_labels, test_labels = convert_labels_to_tensors(train, validation, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2VAmx_2gCb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83620714-44f5-4434-e900-b706a8d2967c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8000, 19825]) torch.Size([1000, 19825]) torch.Size([1000, 19825])\n"
          ]
        }
      ],
      "source": [
        "# This will give us an idea of the dimensions of our generated features.\n",
        "print(train_features_tensor.shape, validation_features_tensor.shape, test_features_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txIfxE58gCb6"
      },
      "source": [
        "## 2. The objective function - cross-entropy loss\n",
        "\n",
        "To optimize the learning process in our classifier, we need to implement a **loss function** or objective function. This function assigns weights to the outputs made by the classifier given how likely they are to be in a class than in another.\n",
        "\n",
        "For a string $x$, we want to learn weights that maximize the probability of the correct label, $p(y \\mid x)$. Since there are only two discrete outcomes ($1$ or $0$), we can make use of the **Bernoulli distribution**, and we can express the probability, $p(y \\mid x)$, that our classifier produces for one observation as the following:\n",
        "$$\n",
        "p(y \\mid x)=\\hat{y}^y(1-\\hat{y})^{1-y} \\quad \\text{where} \\quad \\hat{y}:\\text{predicted label}, \\quad y:\\text{correct label}\n",
        "$$\n",
        "\n",
        "\n",
        "Next, we take the log of both sides. This will turn out to be handy mathematically, and doesn't hurt us; whatever values maximize a probability will also maximize the $\\log$ of the probability:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p(y \\mid x) & =\\log \\left[\\hat{y}^y(1-\\hat{y})^{1-y}\\right] \\\\\n",
        "& =y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To turn this into a loss function (something that we need to minimize), we'll just flip the sign on the equation. The result is the cross-entropy loss $L_{\\mathrm{CE}}$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L_{\\mathrm{CE}}(\\hat{y}, y) & = -\\log p(y \\mid x) \\\\\n",
        "& =-[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Finally, we substitute the loss function in  $\\hat{y}=\\sigma(\\mathbf{w} \\cdot \\mathbf{x}+b)$ to obtain:\n",
        "$$\n",
        "L_{\\mathrm{CE}}(\\hat{y}, y)=-[y \\log \\sigma(\\mathbf{w} \\cdot \\mathbf{x}+b)+(1-y) \\log (1-\\sigma(\\mathbf{w} \\cdot \\mathbf{x}+b))]\n",
        "$$\n",
        "\n",
        "**For a given input, $x$, want the loss to be smaller if the model's estimate is close to correct, and bigger if the model is confused.**\n",
        "\n",
        "The equation above is generalized to:\n",
        "$$\n",
        "L=-\\frac{1}{m} \\sum_{i=1}^m y_i \\cdot \\log \\left(\\hat{y}_i\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-\\hat{y}_i\\right)\\quad \\text{where} \\quad m:\\text{no. of samples in the corpus}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvGTrd4BgCb6"
      },
      "source": [
        "### + 1.0 point - Cross-entropy loss implementation\n",
        "\n",
        "Complete the following function and implement the cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISYeBV_rgCb6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Cross-Entropy Loss\n",
        "def cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    m = len(y_true)\n",
        "    cross_entropy_sum = 0\n",
        "    for y_i_true, y_hat_i_pred in zip(y_true, y_pred):\n",
        "      cross_entropy_sum += ( (y_i_true * np.log(y_hat_i_pred)) + ((1 - y_i_true) * np.log(1 - y_hat_i_pred)) )\n",
        "    cross_entropy_loss_value = ( -1 / m) *  cross_entropy_sum\n",
        "    return cross_entropy_loss_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Fh-dsBgCb6"
      },
      "source": [
        "\n",
        "## 3. The sigmoid function\n",
        "\n",
        "$$ \\begin{align}\n",
        "\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+\\exp (-z)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "- This function is also called the **logistic function**. It takes a real-valued number and maps it into the range $(0,1)$, which is what we want because we want to model probabilities.\n",
        "- The sigmoid function takes real-valued numbers as inputs  --this is why we needed to create the features beforehand.\n",
        "- Because it is nearly linear around $0$ but ﬂattens toward the ends, the sigmoid function tends to squash outlier values toward $0$ or $1$.\n",
        "- It’s differentiable, which is convenient for the learning process (this process is just a series of function optimizations).\n",
        "- Given a input $x$, we want to assign it a label. In the case of our *corpus*, we can only make two decisions: controversial or not controversial.\n",
        "- The sigmoid function is written in terms of $z$ because it takes a **random variable**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1e6mrDIgCb6"
      },
      "source": [
        "### + 1.0 point - Sigmoid function implementation\n",
        "\n",
        "Complete the following function and implement the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cto6oUsxgCb6"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "def sigmoid(x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
        "\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmZToLP4gCb6"
      },
      "source": [
        "## 4. Gradient descent\n",
        "\n",
        "This algorithm will help us **optimize** our loss function. Since our loss function is differentiable, we can optimize it by calculating the derivatives.\n",
        "\n",
        "We now introduce **weights** into our predictions --these should minimize the loss function averaged over all examples:\n",
        "\n",
        "$$\n",
        "L=-\\frac{1}{m} \\sum_{i=1}^m y_i \\cdot \\log \\left(\\sigma\\left(\\mathrm{X}_{\\mathrm{i}} w+b\\right)\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-\\sigma\\left(\\mathrm{X}_{\\mathrm{i}} w+b\\right)\\right)\n",
        "$$\n",
        "\n",
        "By taking the gradient $L$ with respect to $w$, you get the following:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}=\\frac{1}{m}\\left(\\sigma\\left(\\mathrm{X} w+b\\right)-y\\right) X\n",
        "$$\n",
        "\n",
        "By taking the gradient $L$ with respect to $b$, you get the following:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b}=\\frac{1}{m} \\sum_{i=1}^m \\sigma\\left(\\mathrm{X}_{\\mathrm{i}} w+b\\right)-y_i\n",
        "$$\n",
        "\n",
        "[source](https://www.tensorflow.org/guide/core/logistic_regression_core)\n",
        "\n",
        "Stochastic gradient descent is an algorithm that minimizes the loss function by computing its gradient after each training example, and nudging $\\theta$ in the right direction (the opposite direction of the gradient).\n",
        "\n",
        "[source](https://web.stanford.edu/~jurafsky/slp3/5.pdf)\n",
        "\n",
        "\n",
        "The learning rate $\\eta$ is a hyperparameter --this means that it is a value that we have to choose manually and arbitrarily, and adjust later on depending on the optimization results. If it is set too low, the algorithm will take very long to find a minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2G1jFZOgCb6"
      },
      "source": [
        "## 5. Training\n",
        "\n",
        "Now that we have created all the necessary building blocks for our logistic regression model, we will perform the **training phase**, in which we will **learn** the weights for particular *corpus*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm7SM1wDgCb7"
      },
      "source": [
        "### + 1.0 point - Complete the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyKTFSnsgCb7"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "# hyperparameters:\n",
        "# lr: learning rate\n",
        "# epochs: number of passes through the entire corpus.\n",
        "def logistic_regression(X: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 100) -> Tuple[np.ndarray, float]:\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    b = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_pred = sigmoid(z)\n",
        "        loss = cross_entropy_loss(y, y_pred)\n",
        "\n",
        "        # Gradient computation\n",
        "        dw = np.dot(X.T, (y_pred - y)) / m\n",
        "        db = np.sum(y_pred - y) / m\n",
        "\n",
        "        # Update weights\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-9aypx3gCb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50166c0a-8284-4aed-adf4-57dc6e021d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:01<02:25,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931471805600389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 11/100 [00:18<03:27,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.6874562403268131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 21/100 [00:28<01:31,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.6842026284281115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 31/100 [00:39<01:12,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 0.6815564430997848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 41/100 [00:49<01:00,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Loss: 0.6792761469283639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 51/100 [01:00<00:49,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 0.6772473744494814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 61/100 [01:10<00:40,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: 0.6754064689990353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 71/100 [01:21<00:31,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Loss: 0.6737128590671474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 81/100 [01:31<00:21,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Loss: 0.6721383577437882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 91/100 [01:41<00:08,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Loss: 0.6706623396952428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n"
          ]
        }
      ],
      "source": [
        "y = np.array(train_labels)\n",
        "w, b = logistic_regression(train_features_tensor, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0pOFJTgCcA"
      },
      "source": [
        "### 0.5 points - Predict the final probabilities\n",
        "\n",
        "Now, we will create a function that will output the final probabilities of our model. The inputs will be the `trained_model` and the `features` of the test dataset.\n",
        "\n",
        "Fill in the missing code and consider a boundary of $>=0.5$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHpg8TyegCcA"
      },
      "outputs": [],
      "source": [
        "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = sigmoid(z)\n",
        "    for i in range(0, len(y_pred)):\n",
        "      if y_pred[i] >= 0.5:\n",
        "        y_pred[i] = 1\n",
        "      else:\n",
        "        y_pred[i] = 0\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksKGW1-wgCcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42f9b84-b3de-4fc3-8258-a0bc40d54562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
            " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
            " 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
            " 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
            " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "predictions = predict(test_features_tensor, w, b)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCKJbzmOgCcA"
      },
      "source": [
        "## 6. Evaluation metrics\n",
        "\n",
        "A very important step in every ML pipeline is the evaluation since we always want to know how well/bad our model will predict the labels of unseen examples.\n",
        "\n",
        "In NLP we make use of various evaluation metrics for our models. The most common ones are:\n",
        "\n",
        "- Accuracy: It is the ratio of the number of correct predictions divided by the number of total predictions. This gives us an idea of how many correct outputs our model will generate when prediciting labels on unseen examples. The formula is:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Accuracy = \\frac{numberOfCorrectPredictions}{total Number Of Predictions}\n",
        "\\end{aligned}\n",
        "$$\n",
        "- Precision: This metric indicates the ratio of true positive predictions divided by the total number of positive predictions. It indicates us the quality of our model, since it gives us a notion of how many of the predictions in one class actually belong to that class.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Precision = \\frac{truePositives}{total Number Of Predictions}\n",
        "\\end{aligned}\n",
        "$$\n",
        "- Recall: It calculates the proportion of true positive predictions divided by the total number of actual positive items (i.e.,  the sum of true positive and false negative predictions). Intuitively, it tells us how many of the actual positive instances of a class the model correctly identified.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Recall = \\frac{truePositives}{truePositives + false Negatives}\n",
        "\\end{aligned}\n",
        "$$\n",
        "- F1 metric: This metric is the result of dividing precision and recall in an equally weighted manner. It is used to see how balanced are the precision and recall metrics, and when there is an uneven class distribution. In our case, we do not have an unbalanced proportion of labels. What do you expect then to see here for our corpus?\n",
        "$$\n",
        "\\begin{aligned}\n",
        "F1 = 2 * \\frac{Precision*Recall}{Precision+Recall}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjM90gJJgCcA"
      },
      "source": [
        "### + 1.0 pts Evaluation metrics implementation\n",
        "\n",
        "Complete the following functions that generate the evaluation metrics introduced before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK_7OIvRgCcA"
      },
      "outputs": [],
      "source": [
        "#                   | global negative | global positive |\n",
        "#.  system negative |  true negative  |  false negative |\n",
        "#.  system positive |  false positive |  true positive  |\n",
        "#                   |                 |                 |\n",
        "\n",
        "contingency_matrix = np.zeros((2,2))\n",
        "calculated_contingency_matrix = False\n",
        "\n",
        "def calculate_contingency_matrix(y_true: List[int], y_pred: List[int]):\n",
        "  global calculated_contingency_matrix, contingency_matrix\n",
        "  calculated_contingency_matrix = True\n",
        "  contingency_matrix = np.zeros((2,2))\n",
        "  for y_i_true, y_i_pred in zip(y_true, y_pred):\n",
        "    if y_i_true == y_i_pred:\n",
        "      if y_i_true == 0:\n",
        "        contingency_matrix[0][0] += 1 # true negative\n",
        "      else:\n",
        "        contingency_matrix[1][1] += 1 # true positive\n",
        "    else:\n",
        "      if y_i_true == 0:\n",
        "        contingency_matrix[0][1] += 1 # false negative\n",
        "      else:\n",
        "        contingency_matrix[1][0] += 1 # false positive\n",
        "\n",
        "def accuracy(y_true: List[int], y_pred: List[int]) -> float:\n",
        "    # Compute accuracy of the model based on the true and predicted labels\n",
        "    total_predictions = len(y_pred)\n",
        "    calculate_contingency_matrix(y_true, y_pred)\n",
        "    correct_prediction = contingency_matrix[0][0] + contingency_matrix[1][1]\n",
        "    acurac_value = correct_prediction / total_predictions\n",
        "    return acurac_value\n",
        "\n",
        "def precision(y_true: List[int], y_pred: List[int]) -> float:\n",
        "    # Compute precision of the model based on the true and predicted labels\n",
        "    if not calculated_contingency_matrix:\n",
        "      calculate_contingency_matrix(y_true, y_pred)\n",
        "    true_positives = contingency_matrix[1][1]\n",
        "    false_positives = contingency_matrix[1][0]\n",
        "    precision_value = true_positives / (true_positives + false_positives)\n",
        "    return precision_value\n",
        "\n",
        "def recall(y_true: List[int], y_pred: List[int]) -> float:\n",
        "    # Compute recall of the model based on the true and predicted labels\n",
        "    if not calculated_contingency_matrix:\n",
        "      calculate_contingency_matrix(y_true, y_pred)\n",
        "    true_positives = contingency_matrix[1][1]\n",
        "    false_negatives = contingency_matrix[0][1]\n",
        "    recall_value = true_positives / (true_positives + false_negatives)\n",
        "    return recall_value\n",
        "\n",
        "def f1_score(y_true: List[int], y_pred: List[int]) -> float:\n",
        "    # Compute F1 score of the model based on the true and predicted labels\n",
        "    if not calculated_contingency_matrix:\n",
        "      calculate_contingency_matrix(y_true, y_pred)\n",
        "    precision_value = precision(y_true, y_pred)\n",
        "    recall_value = recall(y_true, y_pred)\n",
        "    f1_score_value = 2 * ((precision_value * recall_value) / (precision_value + recall_value))\n",
        "    return f1_score_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46SaPoXsgCcA"
      },
      "source": [
        "## 7. Evaluation\n",
        "\n",
        "Now, we will evaluate the performance of our classifier. During this step we will make use of the test set that we created in the beginning; we will use the test set during this phase because we want to simulate a real-world scenario. That is, if we train a model and we deploy it in a real-world application, we expect that the users will pass new (unseen) text samples to that train model. In our case, those new samples are in the test set. By \"hidding\" the test set during the training phase, we make sure that our models will have some generalization capabilities when dealing with unseen data.\n",
        "\n",
        "### + 0.5 pts - Evaluation\n",
        "\n",
        "Complete the following lines of code and print the classification metrics the trained model achieves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLUwwJS2gCcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf5bca6-8314-485c-d2e7-546632c771c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression with CountVectorizer features - Results:\n",
            "Accuracy: 0.56\n",
            "Precision: 0.484\n",
            "Recall: 0.5707547169811321\n",
            "F1-score 0.5238095238095237\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT ###\n",
        "### Test set results\n",
        "y_true = np.array(test_labels)\n",
        "print('Logistic regression with CountVectorizer features - Results:')\n",
        "print('Accuracy:', accuracy(y_true, predictions))\n",
        "print('Precision:', precision(y_true, predictions))\n",
        "print('Recall:', recall(y_true, predictions))\n",
        "print('F1-score', f1_score(y_true, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "755lZLj7gCcA"
      },
      "source": [
        "## 8. Regularization\n",
        "\n",
        "Even though we want our machine learning models to learn as best as possible from our data, we don't want them to approximate precise functions for all the inputs. Why? Because when we show them new samples, they won't be able to predict its label accurately given that they will only be optimized for a set of specific training samples. Therefore, we want our models to have some room for error.\n",
        "\n",
        "To avoid overfitting, a new regularization term, $R(\\theta)$, is added to the objective function, resulting in the following objective for a batch of $m$ examples (slightly rewritten to be maximizing log probability rather than minimizing loss, and removing the $\\frac{1}{m}$ term which doesn't affect the argmax):\n",
        "$$\n",
        "\\hat{\\theta}=\\underset{\\theta}{\\operatorname{argmax}} \\sum_{i=1}^m \\log P\\left(y^{(i)} \\mid x^{(i)}\\right)-\\alpha R(\\theta)\n",
        "$$\n",
        "\n",
        "The new regularization term, $R(\\theta)$, is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly— but uses many weights with high values to do so-will be penalized more than a setting that matches the data a little less well, but does so using smaller weights. There are two common ways to compute this regularization term $R(\\theta)$. L2 regularization is a quadratic function of the weight values, named because it uses the (square of the) L2 norm of the weight values. The L2 norm, $\\|\\theta\\|_2$, is the same as the Euclidean distance of the vector $\\theta$ from the origin. If $\\theta$ consists of $n$ weights, then:\n",
        "$$\n",
        "R(\\theta)=\\|\\theta\\|_2^2=\\sum_{j=1}^n \\theta_j^2\n",
        "$$\n",
        "\n",
        "The L2 regularized objective function becomes:\n",
        "$$\n",
        "\\hat{\\theta}=\\underset{\\theta}{\\operatorname{argmax}}\\left[\\sum_{i=1}^m \\log P\\left(y^{(i)} \\mid x^{(i)}\\right)\\right]-\\alpha \\sum_{j=1}^n \\theta_j^2\n",
        "$$\n",
        "\n",
        "L1 regularization is a linear function of the weight values, named after the $\\mathrm{L} 1$ norm $\\|W\\|_1$, the sum of the absolute values of the weights, or Manhattan distance (the Manhattan distance is the distance you'd have to walk between two points in a city with a street grid like New York):\n",
        "$$\n",
        "R(\\theta)=\\|\\theta\\|_1=\\sum_{i=1}^n\\left|\\theta_i\\right|\n",
        "$$\n",
        "\n",
        "The L1 regularized objective function becomes:\n",
        "$$\n",
        "\\hat{\\theta}=\\underset{\\theta}{\\operatorname{argmax}}\\left[\\sum_{1=i}^m \\log P\\left(y^{(i)} \\mid x^{(i)}\\right)\\right]-\\alpha \\sum_{j=1}^n\\left|\\theta_j\\right|\n",
        "$$\n",
        "\n",
        "### 1.0 pts - Implement and test the L1 regularization\n",
        "\n",
        "Using the code provided next, train another LR classifier. The difference in this new LR model will be that now we will use the L1 regularization.\n",
        "\n",
        "Evaluate this new LR model and compare the results obtained by this model and the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-c7alQogCcA"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss_l1(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, lambda_: float) -> float:\n",
        "    m = len(y_true)\n",
        "    cross_entropy_sum = 0\n",
        "    for y_i_true, y_hat_i_pred in zip(y_true, y_pred):\n",
        "      cross_entropy_sum += ( (y_i_true * np.log(y_hat_i_pred)) + ((1 - y_i_true) * np.log(1 - y_hat_i_pred)) )\n",
        "    cross_entropy_loss_value = (( -1 / m) * cross_entropy_sum) + (lambda_ * np.sum(np.abs(w)))\n",
        "\n",
        "    return cross_entropy_loss_value\n",
        "\n",
        "def logistic_regression_l1(X: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 100, lambda_: float = 0.1) -> Tuple[np.ndarray, float]:\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    b = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        z = np.dot(X, w) + b\n",
        "        y_pred = sigmoid(z)\n",
        "        loss =  cross_entropy_loss_l1(y, y_pred, w, lambda_)\n",
        "        # Gradient computation\n",
        "        dw = ((1 / m) * np.dot(X.T, (y_pred - y))) + (lambda_ * np.sign(w))\n",
        "\n",
        "        db = np.sum(y_pred - y) / m\n",
        "\n",
        "        # Update weights\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4GuZ-nugCcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0e368c-48e7-46fa-fbbb-b2928d70c98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:01<01:44,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931471805600389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 11/100 [00:11<01:30,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 16.409680530387266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 21/100 [00:22<01:29,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 16.140654550654826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 31/100 [00:33<01:18,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 15.899459211534705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 41/100 [00:44<01:04,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Loss: 15.678296850317624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 51/100 [00:54<00:52,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 15.463073894605348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 61/100 [01:05<00:39,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: 15.252058387860185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 71/100 [01:15<00:29,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Loss: 15.047153109482828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 81/100 [01:26<00:19,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Loss: 14.85189497643821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 91/100 [01:36<00:09,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Loss: 14.661483128990806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
          ]
        }
      ],
      "source": [
        "y = np.array(train_labels)\n",
        "w_l1, b_l1 = logistic_regression_l1(train_features_tensor, y, lr = 0.1, epochs = 100, lambda_ = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_l1 = predict(test_features_tensor, w_l1, b_l1)\n",
        "print(\"Predictions:\", predictions_l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFt0M5wtbgpy",
        "outputId": "8cb97281-7216-42c6-9d4e-223b63213aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array(test_labels)\n",
        "print('Logistic regression with l1 regularization lamda_ 0.1 - Results:')\n",
        "print('Accuracy:', accuracy(y_true, predictions_l1))\n",
        "print('Precision:', precision(y_true, predictions_l1))\n",
        "print('Recall:', recall(y_true, predictions_l1))\n",
        "print('F1-score', f1_score(y_true, predictions_l1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw_ez9R8bmSX",
        "outputId": "b0087e28-ca68-4752-9980-f3f4c9ab670d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression with l1 regularization lamda_ 0.1 - Results:\n",
            "Accuracy: 0.472\n",
            "Precision: 0.074\n",
            "Recall: 0.3627450980392157\n",
            "F1-score 0.12292358803986711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison: LR vs LR with L1\n",
        "\n",
        "| learning rate = 0.1 | Accuracy | Precision | Recall | F1-score | loss (after 100 epochs)\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| LR | 0.560 | 0.484 | 0.5707547169811321 | 0.5238095238095237 | 0.6706623396952428 |\n",
        "| LR with L1 (lambda_ = 0.1) | 0.472 | 0.074 | 0.3627450980392157 | 0.12292358803986711 | 14.661483128990806 |\n",
        "\n",
        "Note: values in the table may change for every run\n",
        "\n",
        "In case of **LR** & **LR with L1**, the loss is higher in **LR with L1** than **LR** that is because of the penalty factor we are adding in the loss function.\n",
        "\n",
        "Also, adding lambda to weight gradient ensures higher weights get penalised and push them towards zero and promoting sparsity. And subtracting the penalty factor ensures negative weights to move towards to zero.\n",
        "\n",
        "Accuracy of **LR with L1** is lesser than **LR**. And it is not as precise as **LR** because of higher loss value. Accuracy of **LR with L1** can be improved by increasing epochs so that the loss of the model becomes smaller i.e. nothing but we converge the model by increasing the train time. With 100 epochs it can be seen that model has not reached it's convergence point."
      ],
      "metadata": {
        "id": "9Cgj37w535Pc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQPNepKgCcA"
      },
      "source": [
        "### + 1.0 point - Comparison with another set of features\n",
        "\n",
        "Using the TF-IDF features we created in the beginning, train a third LR model and compare its performance to the other two.\n",
        "\n",
        "What can you say about these results? Write a brief paragraph where you explain your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oqCUGP4gCcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0171dc-6d70-40f3-97ef-1eb4d319c828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8000, 42593]) torch.Size([1000, 42593]) torch.Size([1000, 42593])\n"
          ]
        }
      ],
      "source": [
        "vocabulary = find_vocabulary(df_subsampled['self_text'])\n",
        "\n",
        "train_docs = generate_docs(train['self_text'])\n",
        "train_tf_idf = compute_tf_idf(train_docs)\n",
        "\n",
        "validation_docs = generate_docs(validation['self_text'])\n",
        "validation_tf_idf = compute_tf_idf(validation_docs)\n",
        "\n",
        "test_docs = generate_docs(test['self_text'])\n",
        "test_tf_idf = compute_tf_idf(test_docs)\n",
        "\n",
        "def generate_feature_matrix(docs_tf_idf: Dict[str, Dict[str, float]]) -> Tuple[torch.Tensor]:\n",
        "  feature_matrix = np.zeros((len(docs_tf_idf), len(vocabulary)))\n",
        "  i = 0\n",
        "  for doc, doc_tf_idf in docs_tf_idf.items():\n",
        "    for token in doc_tf_idf.keys():\n",
        "      feature = token\n",
        "      feature_value = doc_tf_idf[token]\n",
        "      feature_matrix[i][vocabulary.index(feature)] = feature_value\n",
        "    i += 1\n",
        "  return torch.from_numpy(feature_matrix).float()\n",
        "\n",
        "train_features_tf_idf_tensor = generate_feature_matrix(train_tf_idf)\n",
        "validation_features_tf_idf_tensor = generate_feature_matrix(validation_tf_idf)\n",
        "test_features_tf_idf_tensor = generate_feature_matrix(test_tf_idf)\n",
        "print(train_features_tf_idf_tensor.shape, validation_features_tf_idf_tensor.shape, test_features_tf_idf_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(train_labels)\n",
        "w_tf_idf, b_tf_idf = logistic_regression(train_features_tf_idf_tensor, y)"
      ],
      "metadata": {
        "id": "ha6y9nM8TpU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f116e42-0476-47d8-d4db-c449d609ddcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:03<05:15,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931471805600389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 11/100 [00:24<03:11,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.6930118049027276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 21/100 [00:46<02:53,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.6928767110625578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 31/100 [01:08<02:30,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 0.6927418973396542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 41/100 [01:29<02:05,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Loss: 0.6926073622602261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 51/100 [01:51<01:45,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 0.6924731044873189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 61/100 [02:13<01:26,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: 0.692339122767364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 71/100 [02:34<01:00,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Loss: 0.6922054158981544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 81/100 [02:55<00:40,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Loss: 0.6920719827096138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 91/100 [03:17<00:19,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Loss: 0.6919388220523094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:37<00:00,  2.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_tf_idf = predict(test_features_tf_idf_tensor, w_tf_idf, b_tf_idf)\n",
        "print(\"Predictions:\", predictions_tf_idf)"
      ],
      "metadata": {
        "id": "KN9NKa2AfpdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee5d99d-33ab-4d38-c0ef-5505406b22eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0.\n",
            " 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
            " 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
            " 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
            " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
            " 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1.\n",
            " 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
            " 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
            " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array(test_labels)\n",
        "print('Logistic regression with tf-idf features - Results:')\n",
        "print('Accuracy:', accuracy(y_true, predictions_tf_idf))\n",
        "print('Precision:', precision(y_true, predictions_tf_idf))\n",
        "print('Recall:', recall(y_true, predictions_tf_idf))\n",
        "print('F1-score', f1_score(y_true, predictions_tf_idf))"
      ],
      "metadata": {
        "id": "6S4Fjkqlf5Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2dc3642-a849-4002-85e1-386b2766b4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression with tf-idf features - Results:\n",
            "Accuracy: 0.595\n",
            "Precision: 0.772\n",
            "Recall: 0.5701624815361891\n",
            "F1-score 0.6559048428207306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izrfl1OlgCcB"
      },
      "source": [
        "*Write here a few lines explaining your observations from these new results (for example, are the classification metrics higher or lower than the ones previously obtained? Why do you think this happens? Which ones were easier to implement? Which set of features seems more intuitive to you? etc)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "| learning rate = 0.1 | Accuracy | Precision | Recall | F1-score | loss (after 100 epochs)\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| LR | 0.560 | 0.484 | 0.5707547169811321 | 0.5238095238095237 | 0.6706623396952428 |\n",
        "| LR with L1 (lambda_ = 0.1) | 0.472 | 0.074 | 0.3627450980392157 | 0.12292358803986711 | 14.661483128990806 |\n",
        "| LR with tf_idf | 0.595 | 0.772 | 0.5701624815361891 |  0.6559048428207306 | 0.6919388220523094 |\n",
        "\n",
        "Note: values may change for each run\n",
        "\n",
        "Explanation for **LR** and **LR with L1** is provided in the previous section. Therefore, explantion only for **LR with tf_idf** is provided below.\n",
        "\n",
        "1. Accuracy, precision and F1-score of **LR with tf_idf** is higher than the other two models.\n",
        "\n",
        "2. Because, this model generated more loss value in the training phase than **LR**, suggesting **LR with tf_idf** model has more room for error than other models which in turn suggests it is less overfitted than other models. Also, this model generates frequency of each tokens/terms considering entire dataset (idf factor ensures this) unlike in CountVectoriser frequency of the token/terms is determined with respect to particular row/doc only. Hence, td_idf ratio provides a numerical representation that assigns higher values to the most important words in our corpus. Meanwhile, CountVectoriser has an inability to identify more and less important words in the corpus. And unlike tf_idf, it lacks to draw relationship between tokens.\n",
        "\n",
        "3. **LR** and **LR with L1** were easier to implement because they made features built from CountVectoriser. Whereas, there was lot of pre-processing involded in **LR with tf_idf** hence it seemed bit complicated then the rest.\n",
        "\n",
        "4. Feature set generated from tf_idf ratio seemed more intuitive to me than CountVectoriser because of following reasons:\n",
        "    \n",
        "    - It considers whole dataset while determining frequency value and not just the abundanance of token in particular doc/row.\n",
        "    - It identifies the relationships between words such as linguistic similarity between words.\n",
        "    - It has ability to identify more important and less important words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions."
      ],
      "metadata": {
        "id": "ooVNanXfgnHm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzjQbo9zgCcB"
      },
      "source": [
        "### + extra:  1.0 point - LR in sklearn\n",
        "\n",
        "Implement a fourth LR model using the sklearn [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Use the same hyperparameters that you used in the previous trained models. Finally, compare your results with those obtained by the sklearn implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
        "\n",
        "clf = LogisticRegression(penalty=None, random_state=0, max_iter=100)\n",
        "y = np.array(train_labels)\n",
        "y_true = np.array(test_labels)\n",
        "clf.fit(train_features_tensor, y)\n",
        "y_pred = clf.predict(test_features_tensor)\n",
        "total_predictions_sklearn = len(y_pred)\n",
        "\n",
        "y_pred_prob = clf.predict_proba(test_features_tensor)\n",
        "\n",
        "ce_loss = log_loss(y_true, y_pred_prob)\n",
        "accuracy_sklearn = accuracy_score(y_true, y_pred)\n",
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "precision_sklearn = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[0][1])\n",
        "recall_sklearn = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[1][0])\n",
        "f1_score_sklearn = 2 * ((precision_sklearn * recall_sklearn) / (precision_sklearn + recall_sklearn))\n",
        "\n",
        "\n",
        "print(\"Predictions:\", y_pred)\n",
        "print('\\nLogistic regression with sklearn - Results:')\n",
        "print(\"Loss: \" + str(ce_loss))\n",
        "print(\"Accuracy: \" + str(accuracy_sklearn))\n",
        "print(\"Precision: \" + str(precision_sklearn))\n",
        "print(\"Recall: \" + str(recall_sklearn))\n",
        "print(\"F1-score: \" + str(f1_score_sklearn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaFRTFIO5LPO",
        "outputId": "e34a02aa-edd5-4271-ee8c-45228fd6fa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0]\n",
            "\n",
            "Logistic regression with sklearn - Results:\n",
            "Loss: 5.7472093150643815\n",
            "Accuracy: 0.588\n",
            "Precision: 0.584\n",
            "Recall: 0.5887096774193549\n",
            "F1-score: 0.5863453815261045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiL7rWMWgCcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8330ab5d-1616-498b-a5d8-f5c907f178fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0]\n",
            "\n",
            "Logistic regression L1 with sklearn - Results:\n",
            "Loss: 0.6778823276553914\n",
            "Accuracy: 0.565\n",
            "Precision: 0.644\n",
            "Recall: 0.5561312607944733\n",
            "F1-score: 0.5968489341983317\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
        "\n",
        "\n",
        "clf = LogisticRegression(penalty='l1', solver=\"liblinear\", C = 0.1, random_state=0, max_iter=100)\n",
        "y = np.array(train_labels)\n",
        "y_true = np.array(test_labels)\n",
        "clf.fit(train_features_tensor, y)\n",
        "y_pred = clf.predict(test_features_tensor)\n",
        "total_predictions_sklearn = len(y_pred)\n",
        "\n",
        "y_pred_prob = clf.predict_proba(test_features_tensor)\n",
        "\n",
        "ce_loss = log_loss(y_true, y_pred_prob)\n",
        "accuracy_sklearn = accuracy_score(y_true, y_pred)\n",
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "precision_sklearn = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[0][1])\n",
        "recall_sklearn = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[1][0])\n",
        "f1_score_sklearn = 2 * ((precision_sklearn * recall_sklearn) / (precision_sklearn + recall_sklearn))\n",
        "\n",
        "\n",
        "print(\"Predictions:\", y_pred)\n",
        "print('\\nLogistic regression L1 with sklearn - Results:')\n",
        "print(\"Loss: \" + str(ce_loss))\n",
        "print(\"Accuracy: \" + str(accuracy_sklearn))\n",
        "print(\"Precision: \" + str(precision_sklearn))\n",
        "print(\"Recall: \" + str(recall_sklearn))\n",
        "print(\"F1-score: \" + str(f1_score_sklearn))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "| learning rate = 0.1 | Accuracy | Precision | Recall | F1-score | loss (after 100 epochs)\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| LR | 0.560 | 0.484 | 0.5707547169811321 | 0.5238095238095237 | 0.6706623396952428 |\n",
        "| LR with L1 (lambda_ = 0.1) | 0.472 | 0.074 | 0.3627450980392157 | 0.12292358803986711 | 14.661483128990806 |\n",
        "| **LR with tf_idf** | **0.595** | **0.772** | **0.5701624815361891** |  **0.6559048428207306** | **0.6919388220523094** |\n",
        "| LR with Sklearn | 0.588 | 0.584 | 0.5887096774193549 | 0.5863453815261045 | 5.7472093150643815 |\n",
        "| LR with L1 Sklearn | 0.565 | 0.644 | 0.5561312607944733 | 0.5968489341983317 | 0.6778823276553914 |\n",
        "\n",
        "Note: values may change for each run\n",
        "\n",
        "Explanation\n",
        "\n",
        "Above two models represent **LR** and **LR with L1** by initialising penalty to \"none\" and \"l1\" respectively, lambda is set to 0.1 (C=0.1).\n",
        "\n",
        "**LR with Sklearn** has second highest accuracy but it's precision is lower than it's couterpart **LR with L1 Sklearn**. And **LR with Sklearn** has loss of 5.74(approx) which makes it to be less precise than the other one. Also, it lacks in F1-score too.\n",
        "\n",
        "To conclude, **LR with tf_idf** is more accurate among all the models. It also has higher precision, and f1-score. Second best model is **LR with L1 Sklearn**. And **LR with L1** is least accurate of all with highest loss value."
      ],
      "metadata": {
        "id": "gwjC_WIHP9fs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}